{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Query Data\n",
    "First let's connect to Snowflake and grab some data. Enter your snowflake credentials below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import snowflake.connector\n",
    "\n",
    "# Gets the version\n",
    "ctx = snowflake.connector.connect(\n",
    "    user='<USERNAME>@offerupnow.com',\n",
    "    password='<PASSWORD>',\n",
    "    account='offerup',\n",
    ")\n",
    "cs = ctx.cursor()\n",
    "try:\n",
    "    cs.execute(\"SELECT current_version()\")\n",
    "    one = cs.fetchone()\n",
    "    print(one[0])\n",
    "finally:\n",
    "    cs.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "engine = create_engine(\n",
    "    'snowflake://{user}:{password}@{account}/'.format(\n",
    "        user='<USERNAME>@offerupnow.com',\n",
    "        password='<PASSWORD>',\n",
    "        account='offerup',\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "engine = create_engine(\n",
    "    'snowflake://{user}:{password}@{account}/'.format(\n",
    "        user='<USERNAME>@offerupnow.com',\n",
    "        password='<PASSWORD>',\n",
    "        account='offerup',\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Get positive training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open('sql/ebayscam/training-set.sql', 'r') as myfile:\n",
    "    sql=myfile.read()\n",
    "    \n",
    "try:\n",
    "    engine.execute(\"use warehouse analytics_wh;\")\n",
    "    engine.execute(\"use database analytics;\")\n",
    "    engine.execute(\"use schema ebay_scam;\")\n",
    "    engine.execute(\"use role analytics_tool;\")\n",
    "    df = pd.read_sql(sql, engine, 'user_id')\n",
    "finally:\n",
    "    engine.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print df.groupby(['scammer'])['user_agent_length_max'].mean()\n",
    "print df.groupby(['scammer'])['user_agent_length_max'].median()\n",
    "print df.groupby(['scammer'])['user_agent_length_median'].mean()\n",
    "print df.groupby(['scammer'])['user_agent_length_median'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df.groupby(['scammer']).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import dill\n",
    "dill.dump(df, open(\"training-set.pkl\", \"w\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Checkpoint 1\n",
    "At this point, we have serialized a dataset from snowflake to speed up experimental iterations. The data can be loaded from local storage using `dill`, seen below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import dill\n",
    "df = dill.load(open(\"training-set.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Preprocessing\n",
    "Let's prepare the data for using to train a linear model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing as pp\n",
    "import dill\n",
    "\n",
    "X = df.iloc[:,2:]\n",
    "imp = pp.Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "imp.fit(df.iloc[:,2:])\n",
    "dill.dump(imp, open(\"imputer.pkl\", \"w\"))\n",
    "\n",
    "X_imputed = imp.transform(X)\n",
    "y = df.scammer\n",
    "\n",
    "x_scaler = pp.StandardScaler().fit(X_imputed)\n",
    "dill.dump(x_scaler, open(\"x_scaler.pkl\", \"w\"))\n",
    "X_scaled = x_scaler.transform(X_imputed)\n",
    "\n",
    "print X_scaled.mean(axis=0)\n",
    "print X_scaled.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Training Experiments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "nb = GaussianNB()\n",
    "scores = cross_val_score(nb, X_scaled, y)\n",
    "print(\"NB score:\\t  %0.3f\" % scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "lr = LogisticRegression(penalty='l1', class_weight='balanced')\n",
    "scores = cross_val_score(lr, X_scaled, y)\n",
    "print(\"LR score:\\t %0.3f\" % scores.mean() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lr.coef_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Train candidate model and serialize. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import dill\n",
    "\n",
    "lrf = lr.fit(X_scaled, y)\n",
    "score = lrf.score( X_scaled, y)\n",
    "print(\"LR.score():\\t  %0.3f\" % score )\n",
    "dill.dump(lrf, open(\"lr_model.pkl\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "cv = ShuffleSplit(n_splits=3, test_size=0.3, random_state=0)\n",
    "scores = cross_val_score(lrf, X_scaled, y, cv=cv, scoring='f1_macro')\n",
    "\n",
    "print scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Testing\n",
    "Let's query snowflake for users that have registered in the past 7 days. We'll use these users and compare predictions against those users that have already been softblocked. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import sqlalchemy as sa\n",
    "import pandas as pd\n",
    "\n",
    "engine_snowflake = sa.create_engine(\n",
    "    'snowflake://{user}:{password}@{account}/'.format(\n",
    "        user='',\n",
    "        password='',\n",
    "        account='offerup',\n",
    "    ),\n",
    ")\n",
    "\n",
    "try:\n",
    "    new_user_df = pd.read_sql((\"\"\"\n",
    "      SELECT DISTINCT\n",
    "        au.ID                                                                   AS user_id\n",
    "      FROM \"OUBI\".\"PRODPG_PUBLIC\".\"AUTH_USER\" au\n",
    "      WHERE au.IS_ACTIVE\n",
    "            AND au.ID IN (\n",
    "        SELECT e.HEADER_USER_ID AS USER_ID\n",
    "        FROM EVENTS_PROD.PUBLIC.USER_REGISTERED_EVENT e\n",
    "        WHERE datediff('day', e.HEADER_TIMESTAMP, current_timestamp) < %(days)s\n",
    "      )\n",
    "      GROUP BY au.ID\n",
    "      ORDER BY random();\n",
    "        \"\"\"), \n",
    "                    engine_snowflake,\n",
    "                    params={\"days\":\"7\"}\n",
    "                    )\n",
    "finally:\n",
    "    engine_snowflake.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import dill\n",
    "import numpy as np \n",
    "\n",
    "dill.dump(new_user_df, open(\"new_user_cohort.pkl\", \"w\"))\n",
    "new_user_list = dill \\\n",
    "    .load(open(\"new_user_cohort.pkl\")) \\\n",
    "    .user_id \\\n",
    "    .values \\\n",
    "    .tolist()\n",
    "    \n",
    "print new_user_df.shape\n",
    "print len(new_user_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now let's use this new-user cohort to generate features with labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing as pp\n",
    "\n",
    "with open('sql/ebayscam/test-set.sql', 'r') as myfile:\n",
    "    sql=myfile.read()\n",
    "\n",
    "try:    \n",
    "    engine_snowflake.execute(\"use warehouse analytics_wh;\")\n",
    "    engine_snowflake.execute(\"use database analytics;\")\n",
    "    engine_snowflake.execute(\"use schema ebay_scam;\")\n",
    "    engine_snowflake.execute(\"use role analytics_tool;\")\n",
    "    df = pd.read_sql(sql,\n",
    "                     engine_snowflake,\n",
    "                     params={\"ids\":new_user_list[:10000]},\n",
    "                    index_col=['user_id'])\n",
    "finally:\n",
    "    engine_snowflake.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X = df.iloc[:,1:]\n",
    "imp = dill.load(open(\"imputer.pkl\"))\n",
    "X_imputed = imp.transform(X)\n",
    "\n",
    "scaler = dill.load(open(\"x_scaler.pkl\"))\n",
    "X_scaled = scaler.transform(X_imputed)\n",
    "df_x_scaled = pd.DataFrame(data=X_scaled, columns=X.columns,index=X.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import dill\n",
    "\n",
    "lrf = dill.load(open(\"lr_model.pkl\"))\n",
    "\n",
    "yhat = lrf.predict(df_x_scaled)\n",
    "yhat_true_user_id = df_x_scaled[yhat == True].index.values\n",
    "yhat_true_prob = lrf.predict_proba(df_x_scaled)[yhat == True]\n",
    "yhat_true_log_prob = lrf.predict_log_proba(df_x_scaled)[yhat == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "header = 'user_id,prob'\n",
    "X = np.vstack((yhat_true_user_id, yhat_true_prob[:,1]))\n",
    "dill.dump(X, open(\"predictions.pkl\", \"w\"))\n",
    "np.savetxt(\"predictions_prob.csv\", X.T, delimiter=\",\", header=header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Reason Codes\n",
    "Suppose we're interested in understanding the leading features for individual predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def topReasonCodes(cols, scores, n=3):\n",
    "    scores_asc_index = np.argsort(np.absolute(scores))\n",
    "    topThreeCols = cols[scores_asc_index][::-1][:n]\n",
    "    topThreeScores = scores[scores_asc_index][::-1][:n]\n",
    "    reason_codes = tuple(np.vstack((topThreeCols, topThreeScores)))\n",
    "    return reason_codes\n",
    "\n",
    "def calcReasonCode(row, w):\n",
    "    x = row.values\n",
    "    scores =  w.T * x\n",
    "    cols =  row.index.values.astype('str')\n",
    "    return topReasonCodes(cols, scores, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "z = df_x_scaled[yhat == True].apply(calcReasonCode, axis=1, args=(lrf.coef_[0], ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "Top Reason:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "Counter([x[0] for x in z])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "header = 'user_id,prob,reasons,scores'\n",
    "reason_list = [x[0] for x in z]\n",
    "score_list = [x[1] for x in z]\n",
    "X = np.vstack((yhat_true_user_id, yhat_true_prob[:,1], [str(x) for x in reason_list], [str(x) for x in score_list]))\n",
    "np.savetxt(\"predictions_prob_reasons.csv\", X.T, fmt='%s', delimiter=\",\", header=header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### SoftBlocked Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open('sql/ebayscam/softblocked.sql', 'r') as myfile:\n",
    "    sql=myfile.read()\n",
    "\n",
    "try:    \n",
    "    engine_snowflake.execute(\"use warehouse analytics_wh;\")\n",
    "    engine_snowflake.execute(\"use database analytics;\")\n",
    "    engine_snowflake.execute(\"use schema ebay_scam;\")\n",
    "    engine_snowflake.execute(\"use role analytics_tool;\")\n",
    "    df_softblocked = pd.read_sql(sql,\n",
    "                     engine_snowflake,\n",
    "                     params={\"ids\":yhat_true_user_id.tolist()})\n",
    "finally:\n",
    "    engine_snowflake.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "softblocked= [x in df_softblocked.values for x in yhat_true_user_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "header = 'user_id,prob,is_softblocked,reasons,scores'\n",
    "X = np.vstack((yhat_true_user_id, \n",
    "               yhat_true_prob[:,1], \n",
    "               softblocked,\n",
    "               [str(x) for x in reason_list], \n",
    "               [str(x) for x in score_list]))\n",
    "np.savetxt(\"predictions_prob_softblocked_reasons.csv\", X.T, fmt='%s', delimiter=\",\", header=header)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
